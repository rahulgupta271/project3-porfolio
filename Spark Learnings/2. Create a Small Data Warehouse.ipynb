{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Programming Exercise: Create a Small Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create a small data warehouse using Spark to save data as if it were a table in a typical relational database. Once you create this data warehouse, you can query the tables you created using Structured Query Language (SQL).\n",
    "\n",
    "For this exercise, you will execute your Spark SQL within a Python program, but if you are using a typical Hadoop distribution, there are many ways you can connect those tables to existing tools as if it were a normal, relational database. Spark SQL natively supports reading and writing data managed by Apache Hive. Spark can act as a distributed SQL engine allowing you to connect to any tool with JDBC/ODBC support. You can also integrate Spark with big data tools like Apache Phoenix and normal relational databases.\n",
    "\n",
    "For this exercise, you will be creating tables using U.S. Gazetteer files provided by the United States Census Bureau. These files provide a listing of geographic areas for selected areas. You can find the Gazetteer files for 2017 and 2018 in the data directory under the gazetteer folder. These directories contain data for congressional districts, core statistical areas, counties, county subdivisions, schools, census tracts, urban areas, zip codes, and places of interest. You will combine the data from 2017 and 2018, and create tables with the filename of the source (e.g., places.csv is saved in the places table).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gazetteer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Create Unmanaged Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first step of this assignment involves loading the data from the CSV files, combining the file with the file for the other year, and saving it to disk as a table. The following code should provide a template to help you combine tables and save them to the warehouse directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "#import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file paths including filenames\n",
    "file_2017_path = r'/home/ram/share/650/dsc650-master/data/gazetteer/2017/places.csv'\n",
    "\n",
    "file_2018_path = r'/home/ram/share/650/dsc650-master/data/gazetteer/2018/places.csv'\n",
    "\n",
    "warehouse_dir = r'/home/ram/Documents/spark-warehouse/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('week5') \\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_dir) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.load(\n",
    "  file_2017_path,\n",
    "  format='csv',\n",
    "  sep=',',\n",
    "  inferSchema=True,\n",
    "  header=True\n",
    ")\n",
    "\n",
    "df2 = spark.read.load(\n",
    "  file_2018_path,\n",
    "  format='csv',\n",
    "  sep=',',\n",
    "  inferSchema=True,\n",
    "  header=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state='AL', geoid=100100, ansi_code=2582661, name='Abanda CDP', legal_stat_area_desc='57', functional_status='S', land_area_meters_sq=7764034, water_area_meters_sq=34284, land_area_miles_sq=2.998, water_area_miles_sq=0.013000000000000001, latitude=33.091627, longitude=-85.527029, year=2017),\n",
       " Row(state='AL', geoid=100124, ansi_code=2403054, name='Abbeville city', legal_stat_area_desc='25', functional_status='A', land_area_meters_sq=40255362, water_area_meters_sq=107642, land_area_miles_sq=15.543, water_area_miles_sq=0.042, latitude=31.564689, longitude=-85.259124, year=2017),\n",
       " Row(state='AL', geoid=100460, ansi_code=2403063, name='Adamsville city', legal_stat_area_desc='25', functional_status='A', land_area_meters_sq=65211854, water_area_meters_sq=14122, land_area_miles_sq=25.178, water_area_miles_sq=0.005, latitude=33.605748, longitude=-86.974649, year=2017)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.unionAll(df2)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable('places', path = warehouse_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.write.saveAsTable('places', mode = 'overwrite', path = '/home/ram/share/650/spark-warehouse/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each CSV file in the 2017 and 2018 directories, load the data into Spark, combine it with the corresponding data from the other year and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_2017_path = r'/home/ram/share/650/dsc650-master/data/gazetteer/2017'\n",
    "\n",
    "files_2018_path = r'/home/ram/share/650/dsc650-master/data/gazetteer/2018'\n",
    "\n",
    "#os.listdir(\"/home/ram/share/650/dsc650-master/data/gazetteer/2017\")\n",
    "fileslist_2017 = os.listdir(files_2017_path)\n",
    "fileslist_2018 = os.listdir(files_2018_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ram/share/650/dsc650-master/data/gazetteer/2017/congressional_district.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/core_based_statistical_areas.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/counties.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/county_subdivisions.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/elementary_schools.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/places.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/secondary_schools.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/tracts.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/unified_school_districts.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/urban_areas.csv', '/home/ram/share/650/dsc650-master/data/gazetteer/2017/zip_code_tabulation_areas.csv']\n"
     ]
    }
   ],
   "source": [
    "# getting list of files in the directory\n",
    "import glob\n",
    "print(glob.glob(\"/home/ram/share/650/dsc650-master/data/gazetteer/2017/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['congressional_district.csv',\n",
       " 'core_based_statistical_areas.csv',\n",
       " 'counties.csv',\n",
       " 'county_subdivisions.csv',\n",
       " 'elementary_schools.csv',\n",
       " 'places.csv',\n",
       " 'secondary_schools.csv',\n",
       " 'tracts.csv',\n",
       " 'unified_school_districts.csv',\n",
       " 'urban_areas.csv',\n",
       " 'zip_code_tabulation_areas.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of files in 2017 directory\n",
    "fileslist_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['congressional_district.csv',\n",
       " 'core_based_statistical_areas.csv',\n",
       " 'counties.csv',\n",
       " 'county_subdivisions.csv',\n",
       " 'elementary_schools.csv',\n",
       " 'places.csv',\n",
       " 'secondary_schools.csv',\n",
       " 'tracts.csv',\n",
       " 'unified_school_districts.csv',\n",
       " 'urban_areas.csv',\n",
       " 'zip_code_tabulation_areas.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of files in 2018 directory\n",
    "fileslist_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveastable(file, warehouse_dir):\n",
    "    \"\"\"\n",
    "    Combines csv files from different directories (2017 & 2018) and creates a spark table\n",
    "    inputs: 2 - filename, warehouse directory\n",
    "    output: none\n",
    "    expectation: combines files and creates parqurt tables warehouse directory\n",
    "    \"\"\"\n",
    "    \n",
    "    file1_path = os.path.join(files_2017_path,file)\n",
    "    file2_path = os.path.join(files_2018_path,file)\n",
    "    df1 = spark.read.load(\n",
    "      file1_path,\n",
    "      format='csv',\n",
    "      sep=',',\n",
    "      inferSchema=True,\n",
    "      header=True\n",
    "    )\n",
    "\n",
    "    df2 = spark.read.load(\n",
    "      file2_path,\n",
    "      format='csv',\n",
    "      sep=',',\n",
    "      inferSchema=True,\n",
    "      header=True\n",
    "    )\n",
    "\n",
    "    df = df1.unionAll(df2)\n",
    "    \n",
    "    tablename = os.path.splitext(i)[0]\n",
    "    tblwarehouse_dir = os.path.join(warehouse_dir,tablename)\n",
    "    df.write.saveAsTable(tablename, mode = 'overwrite', path = tblwarehouse_dir )\n",
    "    print(\" Table created for - \",tablename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congressional_district.csv\n",
      "congressional_district\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/congressional_district.csv\n",
      " Table created for -  congressional_district\n",
      "core_based_statistical_areas.csv\n",
      "core_based_statistical_areas\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/core_based_statistical_areas.csv\n",
      " Table created for -  core_based_statistical_areas\n",
      "counties.csv\n",
      "counties\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/counties.csv\n",
      " Table created for -  counties\n",
      "county_subdivisions.csv\n",
      "county_subdivisions\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/county_subdivisions.csv\n",
      " Table created for -  county_subdivisions\n",
      "elementary_schools.csv\n",
      "elementary_schools\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/elementary_schools.csv\n",
      " Table created for -  elementary_schools\n",
      "places.csv\n",
      "places\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/places.csv\n",
      " Table created for -  places\n",
      "secondary_schools.csv\n",
      "secondary_schools\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/secondary_schools.csv\n",
      " Table created for -  secondary_schools\n",
      "tracts.csv\n",
      "tracts\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/tracts.csv\n",
      " Table created for -  tracts\n",
      "unified_school_districts.csv\n",
      "unified_school_districts\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/unified_school_districts.csv\n",
      " Table created for -  unified_school_districts\n",
      "urban_areas.csv\n",
      "urban_areas\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/urban_areas.csv\n",
      " Table created for -  urban_areas\n",
      "zip_code_tabulation_areas.csv\n",
      "zip_code_tabulation_areas\n",
      "/home/ram/share/650/dsc650-master/data/gazetteer/2017/zip_code_tabulation_areas.csv\n",
      " Table created for -  zip_code_tabulation_areas\n"
     ]
    }
   ],
   "source": [
    "# loops through each file and merges the 2017 & 2018 data for same file and saves in warehouse directory\n",
    "table_names = []\n",
    "for i in fileslist_2017:\n",
    "    filename = i\n",
    "    tablename = os.path.splitext(i)[0]\n",
    "    table_names.append(tablename)\n",
    "    file_path_name = os.path.join(files_2017_path,i)\n",
    "    print(filename)    \n",
    "    print(tablename)\n",
    "    print(file_path_name)\n",
    "    #call function to merge and create unified parquet file and save it in warehouse directory\n",
    "    saveastable(filename, warehouse_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have finished saving all of the files as tables, verify that you have loaded the files properly by loading the tables into Spark, and performing a simple row count on each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did not use this code, as I have already created tables in the above step\n",
    "def create_external_table(table_name):\n",
    "    table_dir = os.path.join(warehouse_dir, table_name)\n",
    "    return spark.catalog.createExternalTable(table_name, table_dir)\n",
    "\n",
    "def create_external_tables():\n",
    "    for table_name in table_names:\n",
    "        create_external_table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['congressional_district',\n",
       " 'core_based_statistical_areas',\n",
       " 'counties',\n",
       " 'county_subdivisions',\n",
       " 'elementary_schools',\n",
       " 'places',\n",
       " 'secondary_schools',\n",
       " 'tracts',\n",
       " 'unified_school_districts',\n",
       " 'urban_areas',\n",
       " 'zip_code_tabulation_areas']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show existing tables\n",
    "table_names\n",
    "\n",
    "#for i in tablelist:\n",
    "#    create_external_tables() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='congressional_district', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='core_based_statistical_areas', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='counties', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='county_subdivisions', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='elementary_schools', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='places', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='secondary_schools', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='tracts', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='unified_school_districts', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='urban_areas', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='zip_code_tabulation_areas', database='default', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting list of existing tables in spark warehouse\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|    59151|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as row_count from places\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows from table -  congressional_district\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|      880|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  core_based_statistical_areas\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|     1890|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  counties\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|     6440|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  county_subdivisions\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|    73261|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  elementary_schools\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|     3926|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  places\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|    59151|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  secondary_schools\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|      974|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  tracts\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|   148002|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  unified_school_districts\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|    21779|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  urban_areas\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|     7202|\n",
      "+---------+\n",
      "\n",
      "Count of rows from table -  zip_code_tabulation_areas\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|    66288|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing count of rows from each of the merged tables\n",
    "for i in table_names:\n",
    "    sqlstring = \"select count(*) as row_count from \" + i\n",
    "    print(\"Count of rows from table - \", i)\n",
    "    spark.sql(sqlstring).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Load and Query Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have saved the data to external tables, we will load the tables back into Spark and create a report using Spark SQL. For this report, we will create a report on school districts for the states of Nebraska and Iowa using the elementary_schools, secondary_schools and unified_school_districts tables. \n",
    "\n",
    "Using Spark SQL, create a report with the following information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+--------------------+---------+-------------------+----+\n",
      "|state| geoid|                name|lowest_grade_provided|highest_grade_provided|land_area_meters_sq|water_area_meters_sq|land_area_miles_sq| water_area_miles_sq| latitude|          longitude|year|\n",
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+--------------------+---------+-------------------+----+\n",
      "|   AL|100195|Pike Road City Sc...|                   KG|                    12|           81830289|              758331|            31.595|               0.293|32.274993| -86.02787099999999|2017|\n",
      "|   AZ|400004|Clarkdale-Jerome ...|                   PK|                     8|          211351941|              380983| 81.60300000000001|               0.147|34.748857|-112.11202800000001|2017|\n",
      "|   AZ|400005|Salome Consolidat...|                   PK|                     8|         1551915214|               37430|           599.198|0.013999999999999999|33.598773|-113.82446399999999|2017|\n",
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+--------------------+---------+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from elementary_schools\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+-------------------+------------------+-------------------+----+\n",
      "|state| geoid|                name|lowest_grade_provided|highest_grade_provided|land_area_meters_sq|water_area_meters_sq|land_area_miles_sq|water_area_miles_sq|          latitude|          longitude|year|\n",
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+-------------------+------------------+-------------------+----+\n",
      "|   AZ|400082|Colorado River Un...|                    9|                    12|         2711174277|            74672820|           1046.79|             28.831|35.105214000000004|-114.46783400000001|2017|\n",
      "|   AZ|400450|Agua Fria Union H...|                    9|                    12|          244640927|              740286|            94.456|0.28600000000000003|33.481427000000004|        -112.407871|2017|\n",
      "|   AZ|400720|Antelope Union Hi...|                    9|                    12|         7937907675|                3400|          3064.843|              0.001|         32.992049|-113.81737199999999|2017|\n",
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+-------------------+------------------+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from secondary_schools\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+-------------------+---------+----------+----+\n",
      "|state| geoid|                name|lowest_grade_provided|highest_grade_provided|land_area_meters_sq|water_area_meters_sq|land_area_miles_sq|water_area_miles_sq| latitude| longitude|year|\n",
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+-------------------+---------+----------+----+\n",
      "|   AL|100001|Fort Rucker Schoo...|                   KG|                    12|          233007250|             2735224|            89.965|              1.056|31.409737|-85.745807|2017|\n",
      "|   AL|100003|Maxwell AFB Schoo...|                   KG|                    12|            8446722|              566857|             3.261|0.21899999999999997|32.380944|-86.363749|2017|\n",
      "|   AL|100005|Albertville City ...|                   KG|                    12|           68731809|              258708|            26.538|                0.1| 34.26313| -86.21066|2017|\n",
      "+-----+------+--------------------+---------------------+----------------------+-------------------+--------------------+------------------+-------------------+---------+----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from unified_school_districts\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows  3926\n",
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- geoid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lowest_grade_provided: string (nullable = true)\n",
      " |-- highest_grade_provided: integer (nullable = true)\n",
      " |-- land_area_meters_sq: long (nullable = true)\n",
      " |-- water_area_meters_sq: long (nullable = true)\n",
      " |-- land_area_miles_sq: double (nullable = true)\n",
      " |-- water_area_miles_sq: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading spark tables and printing schema\n",
    "elementary_schools_df = spark.read.table(\"elementary_schools\")\n",
    "print(\"Number of rows \", elementary_schools_df.count())\n",
    "elementary_schools_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows  974\n",
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- geoid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lowest_grade_provided: integer (nullable = true)\n",
      " |-- highest_grade_provided: integer (nullable = true)\n",
      " |-- land_area_meters_sq: long (nullable = true)\n",
      " |-- water_area_meters_sq: integer (nullable = true)\n",
      " |-- land_area_miles_sq: double (nullable = true)\n",
      " |-- water_area_miles_sq: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading spark tables and printing schema\n",
    "secondary_schools_df = spark.read.table(\"secondary_schools\")\n",
    "print(\"Number of rows \", secondary_schools_df.count())\n",
    "secondary_schools_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows  21779\n",
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- geoid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lowest_grade_provided: string (nullable = true)\n",
      " |-- highest_grade_provided: integer (nullable = true)\n",
      " |-- land_area_meters_sq: long (nullable = true)\n",
      " |-- water_area_meters_sq: long (nullable = true)\n",
      " |-- land_area_miles_sq: double (nullable = true)\n",
      " |-- water_area_miles_sq: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading spark tables and printing schema\n",
    "unified_school_districts_df = spark.read.table(\"unified_school_districts\")\n",
    "print(\"Number of rows \", unified_school_districts_df.count())\n",
    "unified_school_districts_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es_ilne = spark.sql(\"\"\"select state, year, count(*) as Elementary \n",
    "                                    from elementary_schools \n",
    "                                    where state = 'IA' or state = 'NE'\n",
    "                                    or state = 'NJ'  or state = 'IL'\n",
    "                                    group by state, year\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have joined whole data set based on state and year and not just for those 2 states, as Elementary and Seconday school data sets do not have data for these states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- Elementary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "es_ilne = spark.sql(\"\"\"select state, year, count(*) as Elementary \n",
    "                                    from elementary_schools \n",
    "                                     group by state, year\"\"\").collect()\n",
    "\n",
    "#converting list to data frame\n",
    "es_ilne_df = sc.parallelize(es_ilne).toDF()\n",
    "es_ilne_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+\n",
      "|state|year|Elementary|\n",
      "+-----+----+----------+\n",
      "|   MI|2017|        29|\n",
      "|   WY|2017|         1|\n",
      "|   MI|2018|        28|\n",
      "|   NJ|2017|       171|\n",
      "|   OR|2017|         9|\n",
      "|   VA|2017|         1|\n",
      "|   ME|2018|        11|\n",
      "|   VT|2017|         4|\n",
      "|   RI|2018|         5|\n",
      "|   VA|2018|         1|\n",
      "|   WI|2018|        44|\n",
      "|   WY|2018|         1|\n",
      "|   KY|2018|         5|\n",
      "|   MT|2017|       248|\n",
      "|   IL|2018|       372|\n",
      "|   WI|2017|        46|\n",
      "|   NY|2018|        11|\n",
      "|   TN|2018|        16|\n",
      "|   RI|2017|         5|\n",
      "|   NY|2017|        11|\n",
      "+-----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "es_ilne_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- Secondary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss_ilne = spark.sql(\"\"\"select state, year, count(*) as Secondary \n",
    "                                    from secondary_schools \n",
    "                                     group by state, year\"\"\").collect()\n",
    "#converting list to data frame\n",
    "ss_ilne_df = sc.parallelize(ss_ilne).toDF()\n",
    "ss_ilne_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "|state|year|Secondary|\n",
      "+-----+----+---------+\n",
      "|   NJ|2017|       46|\n",
      "|   OR|2017|        1|\n",
      "|   ME|2018|        6|\n",
      "|   VT|2017|        1|\n",
      "|   RI|2018|        1|\n",
      "|   WI|2018|       10|\n",
      "|   KY|2018|        5|\n",
      "|   MT|2017|      102|\n",
      "|   IL|2018|      105|\n",
      "|   WI|2017|       10|\n",
      "|   NY|2018|        3|\n",
      "|   TN|2018|       16|\n",
      "|   RI|2017|        1|\n",
      "|   NY|2017|        3|\n",
      "|   IL|2017|      102|\n",
      "|   MA|2017|       31|\n",
      "|   TX|2017|        4|\n",
      "|   AZ|2018|       15|\n",
      "|   TX|2018|        3|\n",
      "|   SC|2017|        2|\n",
      "+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss_ilne_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- Unified: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usd_ilne = spark.sql(\"\"\"select state, year, count(*) as Unified \n",
    "                                    from unified_school_districts \n",
    "                                    group by state, year\"\"\").collect()\n",
    "\n",
    "#converting list to data frame\n",
    "usd_ilne_df = sc.parallelize(usd_ilne).toDF()\n",
    "usd_ilne_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|JState|JYear|Unified|\n",
      "+------+-----+-------+\n",
      "|    MI| 2017|    518|\n",
      "|    IA| 2018|    333|\n",
      "|    DE| 2017|     16|\n",
      "|    MD| 2018|     24|\n",
      "|    WY| 2017|     48|\n",
      "|    MI| 2018|    516|\n",
      "|    HI| 2018|      1|\n",
      "|    WA| 2017|    295|\n",
      "|    WA| 2018|    295|\n",
      "|    NE| 2018|    246|\n",
      "|    NV| 2017|     17|\n",
      "|    LA| 2018|     69|\n",
      "|    NJ| 2017|    342|\n",
      "|    IA| 2017|    336|\n",
      "|    OR| 2017|    188|\n",
      "|    VA| 2017|    136|\n",
      "|    PR| 2018|      1|\n",
      "|    ID| 2018|    115|\n",
      "|    KS| 2018|    286|\n",
      "|    AK| 2017|     53|\n",
      "+------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renaming columns to preserve for later stages\n",
    "usd_ilne_df = usd_ilne_df.withColumnRenamed(\"state\",\"JState\")\\\n",
    "                        .withColumnRenamed(\"year\",\"JYear\")\n",
    "usd_ilne_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinexpression =  [usd_ilne_df.JState == es_ilne_df.state, usd_ilne_df.JYear == es_ilne_df.year]\n",
    "joinType = \"left_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+----------+\n",
      "|JState|JYear|Unified|Elementary|\n",
      "+------+-----+-------+----------+\n",
      "|    FL| 2017|     67|      null|\n",
      "|    AL| 2017|    138|         1|\n",
      "|    CT| 2017|    115|        44|\n",
      "|    CO| 2017|    178|      null|\n",
      "|    UT| 2017|     41|      null|\n",
      "|    IL| 2018|    386|       372|\n",
      "|    DC| 2018|      1|      null|\n",
      "|    LA| 2018|     69|      null|\n",
      "|    TN| 2018|    126|        16|\n",
      "|    AR| 2017|    234|      null|\n",
      "|    IN| 2018|    290|      null|\n",
      "|    KS| 2017|    286|      null|\n",
      "|    HI| 2018|      1|      null|\n",
      "|    AZ| 2017|     98|       104|\n",
      "|    DE| 2018|     16|      null|\n",
      "|    RI| 2017|     31|         5|\n",
      "|    DE| 2017|     16|      null|\n",
      "|    MT| 2018|     60|       243|\n",
      "|    MI| 2017|    518|        29|\n",
      "|    NJ| 2018|    342|       171|\n",
      "+------+-----+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reportdf1 = usd_ilne_df.join(es_ilne_df,joinexpression,joinType)#.collect()\n",
    "reportdf1 = reportdf1.drop(\"state\", \"year\")\n",
    "reportdf1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinexpression =  [reportdf1.JState == ss_ilne_df.state, reportdf1.JYear == ss_ilne_df.year]\n",
    "joinType = \"left_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+---------+-------+\n",
      "|State|Year|Elementary|Secondary|Unified|\n",
      "+-----+----+----------+---------+-------+\n",
      "|   AK|2017|      null|     null|     53|\n",
      "|   AK|2018|      null|     null|     53|\n",
      "|   AL|2017|         1|     null|    138|\n",
      "|   AL|2018|         1|     null|    138|\n",
      "|   AR|2017|      null|     null|    234|\n",
      "|   AR|2018|      null|     null|    235|\n",
      "|   AZ|2017|       104|       15|     98|\n",
      "|   AZ|2018|       104|       15|     98|\n",
      "|   CA|2017|       526|      116|    344|\n",
      "|   CA|2018|       524|      112|    345|\n",
      "|   CO|2017|      null|     null|    178|\n",
      "|   CO|2018|      null|     null|    178|\n",
      "|   CT|2017|        44|        8|    115|\n",
      "|   CT|2018|        44|        8|    115|\n",
      "|   DC|2017|      null|     null|      1|\n",
      "|   DC|2018|      null|     null|      1|\n",
      "|   DE|2017|      null|     null|     16|\n",
      "|   DE|2018|      null|     null|     16|\n",
      "|   FL|2017|      null|     null|     67|\n",
      "|   FL|2018|      null|     null|     67|\n",
      "+-----+----+----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reportdf = reportdf1.join(ss_ilne_df,joinexpression,joinType)#.collect()\n",
    "reportdf = reportdf.drop(\"state\", \"year\")\n",
    "\n",
    "reportdf = reportdf.withColumnRenamed(\"JState\",\"State\")\\\n",
    "                        .withColumnRenamed(\"JYear\",\"Year\")\n",
    "\n",
    "reportdf = reportdf.select(\"State\", \"Year\", \"Elementary\", \"Secondary\", \"Unified\")\\\n",
    "                    .sort(\"State\", \"Year\")\n",
    "reportdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the summarized report for all states as spark table in warehouse\n",
    "tablename = 'Allstates_counts'\n",
    "tblwarehouse_dir = os.path.join(warehouse_dir,tablename)\n",
    "reportdf.write.saveAsTable(tablename, mode = 'overwrite', path = tblwarehouse_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+---------+-------+\n",
      "|State|Year|Elementary|Secondary|Unified|\n",
      "+-----+----+----------+---------+-------+\n",
      "|   IA|2017|      null|     null|    336|\n",
      "|   IA|2018|      null|     null|    333|\n",
      "|   NE|2017|      null|     null|    251|\n",
      "|   NE|2018|      null|     null|    246|\n",
      "+-----+----+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using spark SQL analyse the table for few states\n",
    "summary_fewstates = spark.sql(\"\"\"select *\n",
    "                                    from Allstates_counts \n",
    "                                    where state In ('NE' ,'IA') \n",
    "                                    order by state\"\"\")\n",
    "\n",
    "#print the report output for states IA & NE\n",
    "summary_fewstates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Flight Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the previous exercise, you joined data from flights and airport codes to create a report. Create an external table for airport_codes and domestic_flights from the domestic-flights/flights.parquet and airport-codes/airport-codes.csv files. Recreate the report of top ten airports for 2008 using Spark SQL instead of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file paths including filenames\n",
    "parquet_file_path = r'/home/ram/share/650/dsc650-master/data/domestic-flights/flights.parquet'\n",
    "\n",
    "airportdata_filepath = r'/home/ram/share/650/dsc650-master/data/airport-codes/airport-codes.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(origin_airport_code='MHK', destination_airport_code='AMW', origin_city='Manhattan, KS', destination_city='Ames, IA', passengers=21, seats=30, flights=1, distance=254.0, origin_population=122049, destination_population=86219, flight_year=2008, flight_month=10, __index_level_0__=0),\n",
       " Row(origin_airport_code='EUG', destination_airport_code='RDM', origin_city='Eugene, OR', destination_city='Bend, OR', passengers=41, seats=396, flights=22, distance=103.0, origin_population=284093, destination_population=76034, flight_year=1990, flight_month=11, __index_level_0__=1),\n",
       " Row(origin_airport_code='EUG', destination_airport_code='RDM', origin_city='Eugene, OR', destination_city='Bend, OR', passengers=88, seats=342, flights=19, distance=103.0, origin_population=284093, destination_population=76034, flight_year=1990, flight_month=12, __index_level_0__=2),\n",
       " Row(origin_airport_code='EUG', destination_airport_code='RDM', origin_city='Eugene, OR', destination_city='Bend, OR', passengers=11, seats=72, flights=4, distance=103.0, origin_population=284093, destination_population=76034, flight_year=1990, flight_month=10, __index_level_0__=3),\n",
       " Row(origin_airport_code='MFR', destination_airport_code='RDM', origin_city='Medford, OR', destination_city='Bend, OR', passengers=0, seats=18, flights=1, distance=156.0, origin_population=147300, destination_population=76034, flight_year=1990, flight_month=2, __index_level_0__=4)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight = spark.read.parquet(parquet_file_path)\n",
    "df_flight.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft=11.0, continent=None, iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125'),\n",
       " Row(ident='00AA', type='small_airport', name='Aero B Ranch Airport', elevation_ft=3435.0, continent=None, iso_country='US', iso_region='US-KS', municipality='Leoti', gps_code='00AA', iata_code=None, local_code='00AA', coordinates='-101.473911, 38.704022'),\n",
       " Row(ident='00AK', type='small_airport', name='Lowell Field', elevation_ft=450.0, continent=None, iso_country='US', iso_region='US-AK', municipality='Anchor Point', gps_code='00AK', iata_code=None, local_code='00AK', coordinates='-151.695999146, 59.94919968'),\n",
       " Row(ident='00AL', type='small_airport', name='Epps Airpark', elevation_ft=820.0, continent=None, iso_country='US', iso_region='US-AL', municipality='Harvest', gps_code='00AL', iata_code=None, local_code='00AL', coordinates='-86.77030181884766, 34.86479949951172'),\n",
       " Row(ident='00AR', type='closed', name='Newport Hospital & Clinic Heliport', elevation_ft=237.0, continent=None, iso_country='US', iso_region='US-AR', municipality='Newport', gps_code=None, iata_code=None, local_code=None, coordinates='-91.254898, 35.6087')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airpot_codes = spark.read.load(airportdata_filepath, format=\"csv\", sep=\",\", inferschema=True, header=True)\n",
    "\n",
    "df_airpot_codes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join to Origin Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinexpression =  df_flight['origin_airport_code'] == df_airpot_codes['iata_code']\n",
    "joinType = \"left_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(origin_airport_code='MHK', destination_airport_code='AMW', origin_city='Manhattan, KS', destination_city='Ames, IA', passengers=21, seats=30, flights=1, distance=254.0, origin_population=122049, destination_population=86219, flight_year=2008, flight_month=10, type='medium_airport', name='Manhattan Regional Airport', elevation_ft=1057.0, iso_region='US-KS', municipality='Manhattan', gps_code='KMHK', coordinates='-96.6707992553711, 39.14099884033203'),\n",
       " Row(origin_airport_code='EUG', destination_airport_code='RDM', origin_city='Eugene, OR', destination_city='Bend, OR', passengers=41, seats=396, flights=22, distance=103.0, origin_population=284093, destination_population=76034, flight_year=1990, flight_month=11, type='medium_airport', name='Mahlon Sweet Field', elevation_ft=374.0, iso_region='US-OR', municipality='Eugene', gps_code='KEUG', coordinates='-123.21199798583984, 44.12459945678711')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_flight.join(df_airpot_codes,joinexpression,joinType).show(3)\n",
    "df_merged = df_flight.join(df_airpot_codes,joinexpression,joinType)\n",
    "df_merged_modified = df_merged.drop(\"__index_level_0__\",\"ident\",\"local_code\",\"continent\",\"iso_country\",\"iata_code\")\n",
    "df_merged_modified.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_modified2 = df_merged_modified.withColumnRenamed(\"type\",\"origin_airport_type\")\\\n",
    "                                        .withColumnRenamed(\"name\",\"origin_airport_name\")\\\n",
    "                                        .withColumnRenamed(\"elevation_ft\",\"origin_airport_elevation_ft\")\\\n",
    "                                        .withColumnRenamed(\"iso_region\",\"origin_airport_region\")\\\n",
    "                                        .withColumnRenamed(\"municipality\",\"origin_airport_municipality\")\\\n",
    "                                        .withColumnRenamed(\"gps_code\",\"origin_airport_gps_code\")\\\n",
    "                                        .withColumnRenamed(\"coordinates\",\"origin_airport_coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin_airport_code: string (nullable = true)\n",
      " |-- destination_airport_code: string (nullable = true)\n",
      " |-- origin_city: string (nullable = true)\n",
      " |-- destination_city: string (nullable = true)\n",
      " |-- passengers: long (nullable = true)\n",
      " |-- seats: long (nullable = true)\n",
      " |-- flights: long (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- origin_population: long (nullable = true)\n",
      " |-- destination_population: long (nullable = true)\n",
      " |-- flight_year: long (nullable = true)\n",
      " |-- flight_month: long (nullable = true)\n",
      " |-- origin_airport_type: string (nullable = true)\n",
      " |-- origin_airport_name: string (nullable = true)\n",
      " |-- origin_airport_elevation_ft: double (nullable = true)\n",
      " |-- origin_airport_region: string (nullable = true)\n",
      " |-- origin_airport_municipality: string (nullable = true)\n",
      " |-- origin_airport_gps_code: string (nullable = true)\n",
      " |-- origin_airport_coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged_modified2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join to Destination Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinexpression2 =  df_merged_modified2['destination_airport_code'] == df_airpot_codes['iata_code']\n",
    "joinType2 = \"left_outer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged_modified2.join(df_airpot_codes,joinexpression2,joinType2).show(2)\n",
    "df_merged_modified_dest= df_merged_modified2.join(df_airpot_codes,joinexpression2,joinType2)\n",
    "\n",
    "df_merged_modified_dest2 = df_merged_modified_dest.drop(\"__index_level_0__\",\"ident\",\"local_code\",\"continent\",\"iso_country\")\n",
    "\n",
    "\n",
    "df_merged_modified_dest_final = df_merged_modified_dest2.withColumnRenamed(\"type\",\"destination_airport_type\")\\\n",
    "                                        .withColumnRenamed(\"name\",\"destination_airport_name\")\\\n",
    "                                        .withColumnRenamed(\"elevation_ft\",\"destination_airport_elevation_ft\")\\\n",
    "                                        .withColumnRenamed(\"iso_region\",\"destination_airport_region\")\\\n",
    "                                        .withColumnRenamed(\"municipality\",\"destination_airport_municipality\")\\\n",
    "                                        .withColumnRenamed(\"gps_code\",\"destination_airport_gps_code\")\\\n",
    "                                        .withColumnRenamed(\"coordinates\",\"destination_airport_coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin_airport_code: string (nullable = true)\n",
      " |-- destination_airport_code: string (nullable = true)\n",
      " |-- origin_city: string (nullable = true)\n",
      " |-- destination_city: string (nullable = true)\n",
      " |-- passengers: long (nullable = true)\n",
      " |-- seats: long (nullable = true)\n",
      " |-- flights: long (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- origin_population: long (nullable = true)\n",
      " |-- destination_population: long (nullable = true)\n",
      " |-- flight_year: long (nullable = true)\n",
      " |-- flight_month: long (nullable = true)\n",
      " |-- origin_airport_type: string (nullable = true)\n",
      " |-- origin_airport_name: string (nullable = true)\n",
      " |-- origin_airport_elevation_ft: double (nullable = true)\n",
      " |-- origin_airport_region: string (nullable = true)\n",
      " |-- origin_airport_municipality: string (nullable = true)\n",
      " |-- origin_airport_gps_code: string (nullable = true)\n",
      " |-- origin_airport_coordinates: string (nullable = true)\n",
      " |-- destination_airport_type: string (nullable = true)\n",
      " |-- destination_airport_name: string (nullable = true)\n",
      " |-- destination_airport_elevation_ft: double (nullable = true)\n",
      " |-- destination_airport_region: string (nullable = true)\n",
      " |-- destination_airport_municipality: string (nullable = true)\n",
      " |-- destination_airport_gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- destination_airport_coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged_modified_dest_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Ten Airports By Inbound Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_modified_dest_final.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(origin_airport_code='MHK', destination_airport_code='AMW', origin_city='Manhattan, KS', destination_city='Ames, IA', passengers=21, seats=30, flights=1, distance=254.0, origin_population=122049, destination_population=86219, flight_year=2008, flight_month=10, origin_airport_type='medium_airport', origin_airport_name='Manhattan Regional Airport', origin_airport_elevation_ft=1057.0, origin_airport_region='US-KS', origin_airport_municipality='Manhattan', origin_airport_gps_code='KMHK', origin_airport_coordinates='-96.6707992553711, 39.14099884033203', destination_airport_type='small_airport', destination_airport_name='Ames Municipal Airport', destination_airport_elevation_ft=956.0, destination_airport_region='US-IA', destination_airport_municipality='Ames', destination_airport_gps_code='KAMW', iata_code='AMW', destination_airport_coordinates='-93.621803, 41.992001'),\n",
       " Row(origin_airport_code='SEA', destination_airport_code='RDM', origin_city='Seattle, WA', destination_city='Bend, OR', passengers=126, seats=148, flights=4, distance=228.0, origin_population=6713274, destination_population=157730, flight_year=2008, flight_month=11, origin_airport_type='large_airport', origin_airport_name='Seattle Tacoma International Airport', origin_airport_elevation_ft=433.0, origin_airport_region='US-WA', origin_airport_municipality='Seattle', origin_airport_gps_code='KSEA', origin_airport_coordinates='-122.308998, 47.449001', destination_airport_type='medium_airport', destination_airport_name='Roberts Field', destination_airport_elevation_ft=3080.0, destination_airport_region='US-OR', destination_airport_municipality='Redmond', destination_airport_gps_code='KRDM', iata_code='RDM', destination_airport_coordinates='-121.1500015, 44.2541008')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with data from 2008\n",
    "df_2008 = spark.sql(\"SELECT * FROM dfTable where flight_year = 2008\")\n",
    "df_2008.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataframe with 2008 as a table\n",
    "df_2008.write.saveAsTable('Flight_2008_data', mode = 'overwrite', \n",
    "                          path = '/home/ram/Documents/spark-warehouse/flight_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with data from 2008\n",
    "Top10 = spark.sql(\"\"\" SELECT * FROM \n",
    "                    (\n",
    "                    SELECT  Airport_Name,\n",
    "                            Airpot_Code,\n",
    "                            dense_rank() Over(ORDER BY Total_Inbound_Passengers DESC) as Rank,\n",
    "                            Total_Inbound_Passengers,\n",
    "                            Total_Inbound_Flights,\n",
    "                            Average_Daily_Passengers,\n",
    "                            Average_DailyFlights\n",
    "                        FROM \n",
    "                        (\n",
    "                            SELECT \n",
    "                            destination_airport_name as Airport_Name, \n",
    "                            destination_airport_code as Airpot_Code,  \n",
    "                            count(passengers) as Total_Inbound_Passengers,\n",
    "                            count(flights) as Total_Inbound_Flights,\n",
    "                            mean(passengers) as Average_Daily_Passengers,\n",
    "                            mean(flights) as Average_DailyFlights                            \n",
    "                            FROM Flight_2008_data \n",
    "                            GROUP BY destination_airport_name, destination_airport_code\n",
    "                            ) Temp\n",
    "                      ) RankedTable \n",
    "                          WHERE Rank < 11\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----+------------------------+---------------------+------------------------+--------------------+\n",
      "|        Airport_Name|Airpot_Code|Rank|Total_Inbound_Passengers|Total_Inbound_Flights|Average_Daily_Passengers|Average_DailyFlights|\n",
      "+--------------------+-----------+----+------------------------+---------------------+------------------------+--------------------+\n",
      "|Chicago O'Hare In...|        ORD|   1|                    9479|                 9479|      2784.9765798079966|   37.61683721911594|\n",
      "|Hartsfield Jackso...|        ATL|   2|                    8775|                 8775|       4052.626210826211|   45.03612535612535|\n",
      "|Charlotte Douglas...|        CLT|   3|                    6152|                 6152|      2444.4878088426526|   33.32899869960988|\n",
      "|Minneapolis-St Pa...|        MSP|   4|                    5988|                 5988|      2354.3926185704745|  29.740313961255843|\n",
      "|Philadelphia Inte...|        PHL|   5|                    5880|                 5880|      2193.2382653061222|   32.08163265306123|\n",
      "|George Bush Inter...|        IAH|   6|                    5811|                 5811|      2559.0633281707105|   36.86886938564791|\n",
      "|Detroit Metropoli...|        DTW|   7|                    5638|                 5638|       2523.747250798155|   34.03866619368571|\n",
      "|Memphis Internati...|        MEM|   8|                    5549|                 5549|       895.4296269598126|   25.22526581366012|\n",
      "|Dallas Fort Worth...|        DFW|   9|                    5109|                 5109|       4479.067919357995|   52.89547856723429|\n",
      "|Cincinnati Northe...|        CVG|  10|                    4821|                 4821|      1245.3727442439329|  25.121344119477286|\n",
      "+--------------------+-----------+----+------------------------+---------------------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Top10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
