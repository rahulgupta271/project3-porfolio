{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create text classification models using logistic regression & Naive Bayes and evaluate the performance of those models through Cross Validation and Parameter Tuning to select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data set and place it in the appropriate directory. The zip file of data set is available on university site - https://content.bellevue.edu/cst/dsc/550/id/data.zip.  The JSONL data files for this project are in reddit directory. Place the \"controversial-comments.jsonl\" and \"categorized-comments.jsonl\" in the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries for dataset preparation, feature engineering, model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string, numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "# for measuring accuracy, precision, recall, f1 and auc scores\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   con                                                txt\n",
      "0    0  Well it's great that he did something about th...\n",
      "1    0                       You are right Mr. President.\n",
      "2    0  You have given no input apart from saying I am...\n",
      "3    0  I get the frustration but the reason they want...\n",
      "4    0  I am far from an expert on TPP and I would ten...\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_json('controversial-comments.jsonl', lines = 'True')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950000\n",
      "count of 0 contro : 908145\n",
      "count of 1 contro : 41855\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(\"count of 0 contro :\", len(df[df.con == 0]))\n",
    "print(\"count of 1 contro :\", len(df[df.con == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text\n",
    "def textcleaning(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    #removing \\n\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # removing urls\n",
    "    text = re.sub(r'http.?://[^\\s]+[\\s]?', ' ', text)\n",
    "    # removing symbols and numbers\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    # removing 3 letter words\n",
    "    text = re.sub(r'(\\b\\w{1,3}\\b)', '', text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying text cleaning on text field to clean it up\n",
    "df['clndtxt'] = df['txt'].apply(textcleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmedtxt'] = df['clndtxt'].apply(lemmatize_text).apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "      <th>clndtxt</th>\n",
       "      <th>lemmedtxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it's great that he did something about th...</td>\n",
       "      <td>well  great that   something about those belie...</td>\n",
       "      <td>well great that something about those belief w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You are right Mr. President.</td>\n",
       "      <td>right  president</td>\n",
       "      <td>right president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>You have given no input apart from saying I am...</td>\n",
       "      <td>have given  input apart from saying   wrong  ...</td>\n",
       "      <td>have given input apart from saying wrong have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I get the frustration but the reason they want...</td>\n",
       "      <td>frustration   reason they want them    that...</td>\n",
       "      <td>frustration reason they want them that because...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I am far from an expert on TPP and I would ten...</td>\n",
       "      <td>from  expert     would tend  agree that    ...</td>\n",
       "      <td>from expert would tend agree that problem unde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt  \\\n",
       "0    0  Well it's great that he did something about th...   \n",
       "1    0                       You are right Mr. President.   \n",
       "2    0  You have given no input apart from saying I am...   \n",
       "3    0  I get the frustration but the reason they want...   \n",
       "4    0  I am far from an expert on TPP and I would ten...   \n",
       "\n",
       "                                             clndtxt  \\\n",
       "0  well  great that   something about those belie...   \n",
       "1                                   right  president   \n",
       "2   have given  input apart from saying   wrong  ...   \n",
       "3     frustration   reason they want them    that...   \n",
       "4     from  expert     would tend  agree that    ...   \n",
       "\n",
       "                                           lemmedtxt  \n",
       "0  well great that something about those belief w...  \n",
       "1                                    right president  \n",
       "2  have given input apart from saying wrong have ...  \n",
       "3  frustration reason they want them that because...  \n",
       "4  from expert would tend agree that problem unde...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "      <th>clndtxt</th>\n",
       "      <th>lemmedtxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it's great that he did something about th...</td>\n",
       "      <td>well great that something about those belief w...</td>\n",
       "      <td>well great that something about those belief w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You are right Mr. President.</td>\n",
       "      <td>right president</td>\n",
       "      <td>right president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>You have given no input apart from saying I am...</td>\n",
       "      <td>have given input apart from saying wrong have ...</td>\n",
       "      <td>have given input apart from saying wrong have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I get the frustration but the reason they want...</td>\n",
       "      <td>frustration reason they want them that because...</td>\n",
       "      <td>frustration reason they want them that because...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I am far from an expert on TPP and I would ten...</td>\n",
       "      <td>from expert would tend agree that problem unde...</td>\n",
       "      <td>from expert would tend agree that problem unde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt  \\\n",
       "0    0  Well it's great that he did something about th...   \n",
       "1    0                       You are right Mr. President.   \n",
       "2    0  You have given no input apart from saying I am...   \n",
       "3    0  I get the frustration but the reason they want...   \n",
       "4    0  I am far from an expert on TPP and I would ten...   \n",
       "\n",
       "                                             clndtxt  \\\n",
       "0  well great that something about those belief w...   \n",
       "1                                    right president   \n",
       "2  have given input apart from saying wrong have ...   \n",
       "3  frustration reason they want them that because...   \n",
       "4  from expert would tend agree that problem unde...   \n",
       "\n",
       "                                           lemmedtxt  \n",
       "0  well great that something about those belief w...  \n",
       "1                                    right president  \n",
       "2  have given input apart from saying wrong have ...  \n",
       "3  frustration reason they want them that because...  \n",
       "4  from expert would tend agree that problem unde...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the column to latest transformed values\n",
    "df['clndtxt'] = df['lemmedtxt']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, target = df['clndtxt'], df['con']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "text_train, text_test, label_train, label_test = model_selection.train_test_split(df['clndtxt'], df['con'], \n",
    "                                                                                  test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in train 712500\n",
      "Number of observations in train 237500\n"
     ]
    }
   ],
   "source": [
    "# Checking lenghts of test and test\n",
    "print(\"Number of observations in train\", len(text_train))\n",
    "\n",
    "print(\"Number of observations in train\", len(text_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79527\n"
     ]
    }
   ],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', stop_words='english', lowercase=True, min_df = 2)\n",
    "\n",
    "cvtrnsftexttrain = count_vect.fit_transform(text_train)\n",
    "print(len(count_vect.get_feature_names())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712500, 79527)\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.fit_transform(text_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237500, 79527)\n"
     ]
    }
   ],
   "source": [
    "# converting testing text into matrix form for computation using transform\n",
    "cvtrnsftexttest = count_vect.transform(text_test)\n",
    "print(count_vect.transform(text_test).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712500, 79527)\n",
      "(237500, 79527)\n"
     ]
    }
   ],
   "source": [
    "# Confirming shapes of test and train transformations\n",
    "print(cvtrnsftexttrain.shape)\n",
    "print(cvtrnsftexttest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistict Regression with penalty = L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
       "          penalty='l1', random_state=0, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating instance for logistic regression with penalty L1\n",
    "logregmodel = LogisticRegression(n_jobs=-1, penalty='l1', random_state=0) \n",
    "\n",
    "\n",
    "# Applying logistic regression to train data set\n",
    "logregmodel.fit(cvtrnsftexttrain, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on trained dataset\n",
    "logregmodelpredtr = logregmodel.predict(cvtrnsftexttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on test dataset\n",
    "logregmodelpredtest = logregmodel.predict(cvtrnsftexttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226766\n"
     ]
    }
   ],
   "source": [
    "# Testing total accurate predictions\n",
    "print (np.sum(logregmodelpredtest == label_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9548042105263158\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(logregmodelpredtest == label_test)/237500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98    226968\n",
      "           1       0.15      0.00      0.01     10532\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    237500\n",
      "   macro avg       0.55      0.50      0.49    237500\n",
      "weighted avg       0.92      0.95      0.93    237500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate labelled performance metrics\n",
    "print(metrics.classification_report(label_test, logregmodelpredtest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9564729824561403 0.9548042105263158\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = metrics.accuracy_score(label_train, logregmodelpredtr)\n",
    "accuracy_test = metrics.accuracy_score(label_test, logregmodelpredtest)\n",
    "print(accuracy_train, accuracy_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rows list for data manipulation\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_metrics(actual_label, predicted_label):\n",
    "    \"\"\"\n",
    "    function that takes acutal labels and predicted labels and returns\n",
    "    accuracy, auc, precision, recall and f1 scores\n",
    "    average = 'micro' - to return global metrics\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(actual_label, predicted_label)\n",
    "    auc =  roc_auc_score(actual_label, predicted_label)\n",
    "    precision = precision_score(actual_label, predicted_label, average = 'micro')\n",
    "    recall = recall_score(actual_label, predicted_label,  average = 'micro')\n",
    "    f1 = f1_score(actual_label, predicted_label, average = 'micro')\n",
    "    \n",
    "    return (accuracy, auc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression (L1)', 0.9548042105263158, 0.5015922168040325, 0.9548042105263158, 0.9548042105263158, 0.9548042105263158]\n"
     ]
    }
   ],
   "source": [
    "#print(fn_metrics(label_test, logregmodelpredtest))\n",
    "\n",
    "acc, auc, prec, recall, f1 = fn_metrics(label_test, logregmodelpredtest)\n",
    "\n",
    "l1row= ['Logistic Regression (L1)', acc, auc, prec, recall, f1]\n",
    "print(l1row)\n",
    "\n",
    "rows.append(l1row)\n",
    "                    \n",
    "# ['Logistic Regression (L1)', 0.9548042105263158, 0.5015922168040325, 0.1541095890410959, 0.0042726927459172045, 0.008314855875831485]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Penalty = L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
       "          penalty='l2', random_state=0, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregmodel2 = LogisticRegression(n_jobs=-1, penalty='l2', random_state=0, solver = 'sag')\n",
    "logregmodel2.fit(cvtrnsftexttrain, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98    226968\n",
      "           1       0.24      0.00      0.00     10532\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    237500\n",
      "   macro avg       0.60      0.50      0.49    237500\n",
      "weighted avg       0.92      0.96      0.93    237500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on trained dataset\n",
    "logregmodelpredtr2 = logregmodel2.predict(cvtrnsftexttrain)\n",
    "\n",
    "# predict the labels on test dataset\n",
    "logregmodelpredtest2 = logregmodel2.predict(cvtrnsftexttest)\n",
    "\n",
    "print(metrics.classification_report(label_test, logregmodelpredtest2)) #, target_names=docs_to_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559705263157895 0.9556\n"
     ]
    }
   ],
   "source": [
    "accuracy_trainl2 = metrics.accuracy_score(label_train, logregmodelpredtr2)\n",
    "accuracy_testl2 = metrics.accuracy_score(label_test, logregmodelpredtest2)\n",
    "print(accuracy_trainl2, accuracy_testl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression (L2)', 0.9556, 0.5002429900623745, 0.9556, 0.9556, 0.9556]\n"
     ]
    }
   ],
   "source": [
    "#print(fn_metrics(label_test, logregmodelpredtest))\n",
    "\n",
    "acc, auc, prec, recall, f1 = fn_metrics(label_test, logregmodelpredtest2)\n",
    "\n",
    "l2row= ['Logistic Regression (L2)', acc, auc, prec, recall, f1]\n",
    "print(l2row)\n",
    "\n",
    "rows.append(l2row)\n",
    "# ['Logistic Regression (L2)', 0.9556, 0.5002429900623745, 0.24, 0.000569692366122294, 0.0011366865586814436]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428533333333333 0.9420547368421053\n",
      "223738\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "Nb_cv = MultinomialNB()\n",
    "Nb_cv.fit(cvtrnsftexttrain, label_train)\n",
    "\n",
    "# predict the labels on trained dataset\n",
    "NB_cv_predtr = Nb_cv.predict(cvtrnsftexttrain)\n",
    "\n",
    "# predict the labels on test dataset\n",
    "NB_cv_predtst = Nb_cv.predict(cvtrnsftexttest)\n",
    "\n",
    "# Calculating Accuracy scores\n",
    "Acc_NB_cv_predtr = metrics.accuracy_score(label_train, NB_cv_predtr)\n",
    "Acc_NB_cv_predtst = metrics.accuracy_score(label_test, NB_cv_predtst)\n",
    "print(Acc_NB_cv_predtr, Acc_NB_cv_predtst) \n",
    "\n",
    "# Testing total accurate predictions for test\n",
    "print (np.sum(NB_cv_predtst == label_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Naive Bayes', 0.9420547368421053, 0.5124417086564873, 0.9420547368421053, 0.9420547368421053, 0.9420547368421053]\n"
     ]
    }
   ],
   "source": [
    "#print(fn_metrics(label_test, logregmodelpredtest))\n",
    "\n",
    "acc, auc, prec, recall, f1 = fn_metrics(label_test, NB_cv_predtst)\n",
    "\n",
    "nbrow= ['Naive Bayes', acc, auc, prec, recall, f1]\n",
    "print(nbrow)\n",
    "\n",
    "rows.append(nbrow)\n",
    "# ['Naive Bayes', 0.9420547368421053, 0.5124417086564873, 0.10552027357107963, 0.041017850360805165, 0.05907288390537399]                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data frame for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (L1)</td>\n",
       "      <td>0.954804</td>\n",
       "      <td>0.501592</td>\n",
       "      <td>0.954804</td>\n",
       "      <td>0.954804</td>\n",
       "      <td>0.954804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression (L2)</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.500243</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.942055</td>\n",
       "      <td>0.512442</td>\n",
       "      <td>0.942055</td>\n",
       "      <td>0.942055</td>\n",
       "      <td>0.942055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy       AUC  Precision    Recall        F1\n",
       "0  Logistic Regression (L1)  0.954804  0.501592   0.954804  0.954804  0.954804\n",
       "1  Logistic Regression (L2)  0.955600  0.500243   0.955600  0.955600  0.955600\n",
       "2               Naive Bayes  0.942055  0.512442   0.942055  0.942055  0.942055"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data frame to hold results\n",
    "modelperf_df = pd.DataFrame()\n",
    "modelperf_df = pd.DataFrame(rows, columns = ['Model', 'Accuracy', 'AUC', 'Precision', 'Recall', 'F1'])\n",
    "modelperf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisitc Regression penalty L2 has slightly better accuracy than other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Create k-Fold cross-validation \n",
    "kf3 = KFold(n_splits=3 # 3 fold cross validation\n",
    "            , shuffle=True # to shuffle observations\n",
    "            ,random_state=1)\n",
    "\n",
    "#features = cvtrnsftexttrain #cvtrnsftexttest\n",
    "#target = label_train #label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96711\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text, target = df['clndtxt'], df['con']\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', stop_words='english', lowercase=True, min_df = 2)\n",
    "\n",
    "# Converting features into vectorized matrix for applying model\n",
    "features = count_vect.fit_transform(text)\n",
    "print(len(count_vect.get_feature_names())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct k-fold cross-validation\n",
    "cv_results = cross_val_score(logregmodel2, # Logistic Regression with penalty L2\n",
    "                            features, # Feature matrix\n",
    "                            target, # Target vector\n",
    "                            cv=kf3, # Cross-validation technique\n",
    "                            scoring=\"roc_auc\", # Loss function\n",
    "                            n_jobs=-1) # Use all CPU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63102262 0.60787666 0.60858587]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)\n",
    "# for test [0.5289055  0.53574204 0.60549728]\n",
    "# for train [0.62508317 0.61615647 0.6207463 ]\n",
    "# for entire data set [0.63102262 0.60787666 0.60858587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6158283839346127\n"
     ]
    }
   ],
   "source": [
    "print(cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation improved AUC of Logistic Regression (L2) from  0.500668 to  0.615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Label Classification using Logistic Regression and  Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cat                                                txt\n",
      "0  sports  Barely better than Gabbert? He was significant...\n",
      "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
      "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
      "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
      "4  sports                                      No!! NOO!!!!!\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "catdf = pd.read_json('categorized-comments.jsonl', lines = 'True')\n",
    "print(catdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat    2347476\n",
       "txt    2347476\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'science_and_technology', 'video_games', 'news'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique values for cat\n",
    "catdf['cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>408311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science_and_technology</th>\n",
       "      <td>158246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>775199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_games</th>\n",
       "      <td>1005720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            txt\n",
       "cat                            \n",
       "news                     408311\n",
       "science_and_technology   158246\n",
       "sports                   775199\n",
       "video_games             1005720"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether categorizations are balanced or not by checking counts for each categorization\n",
    "catdf.groupby('cat').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying original data frame\n",
    "catdf_orig = catdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like categorizations are imbalanced, so need to add logic to balance that by adding class_weight=\"balanced\" parameter and as it is multi-classification add multi_class = 'multinomial' for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>775199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         txt\n",
       "cat         \n",
       "0     408311\n",
       "1     158246\n",
       "2     775199\n",
       "3    1005720"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting \"cat\" column to numerical encoding\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "cat = catdf['cat']\n",
    "catdf['cat'] = encoder.fit_transform(cat)\n",
    "catdf.groupby('cat').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat                                                txt\n",
       "0    2  Barely better than Gabbert? He was significant...\n",
       "1    2  Fuck the ducks and the Angels! But welcome to ...\n",
       "2    2  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3    2            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4    2                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to performance issues in handling 2.3M rows, using proportionate sampling to complete model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117374"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling huge data set and at the same time, keep the fraction of the category by using same samplng fraction across\n",
    "\n",
    "catdf0 = catdf[catdf.cat == 0 ].sample(frac=0.05, random_state= 1)\n",
    "#len(catdf0)\n",
    "catdf1 = catdf[catdf.cat == 1 ].sample(frac=0.05, random_state= 1)\n",
    "catdf2 = catdf[catdf.cat == 2 ].sample(frac=0.05, random_state= 1)\n",
    "catdf3 = catdf[catdf.cat == 3 ].sample(frac=0.05, random_state= 1)\n",
    "\n",
    "\n",
    "sampledcatdf = pd.concat([catdf0, catdf1, catdf2, catdf3])\n",
    "len(sampledcatdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       txt\n",
       "cat       \n",
       "0    20416\n",
       "1     7912\n",
       "2    38760\n",
       "3    50286"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledcatdf.groupby('cat').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text\n",
    "def textcleaning(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    #removing \\n\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # removing urls\n",
    "    text = re.sub(r'http.?://[^\\s]+[\\s]?', ' ', text)\n",
    "    # removing symbols and numbers\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    # removing 3 letter words\n",
    "    text = re.sub(r'(\\b\\w{1,3}\\b)', '', text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying cleaning on sampled data set\n",
    "sampledcatdf['clndtxt'] = sampledcatdf['txt'].apply(textcleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>clndtxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1553277</th>\n",
       "      <td>0</td>\n",
       "      <td>#\"The Main Obstacle To A Stable And Just World...</td>\n",
       "      <td>main obstacle   stable  just world order   un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070434</th>\n",
       "      <td>0</td>\n",
       "      <td>It might be you</td>\n",
       "      <td>might</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498674</th>\n",
       "      <td>0</td>\n",
       "      <td>His PR team is on it right now. Big time.</td>\n",
       "      <td>team    right   time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244475</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;gt; My point is that the West Bank is part of...</td>\n",
       "      <td>point  that  west bank  part  israel  everyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055758</th>\n",
       "      <td>0</td>\n",
       "      <td>Sadly someone who wishes to keep you in a stat...</td>\n",
       "      <td>sadly someone  wishes  keep    state  ignoranc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat                                                txt  \\\n",
       "1553277    0  #\"The Main Obstacle To A Stable And Just World...   \n",
       "2070434    0                                    It might be you   \n",
       "1498674    0         His PR team is on it right now. Big time.    \n",
       "2244475    0  &gt; My point is that the West Bank is part of...   \n",
       "2055758    0  Sadly someone who wishes to keep you in a stat...   \n",
       "\n",
       "                                                   clndtxt  \n",
       "1553277   main obstacle   stable  just world order   un...  \n",
       "2070434                                            might    \n",
       "1498674                              team    right   time   \n",
       "2244475    point  that  west bank  part  israel  everyt...  \n",
       "2055758  sadly someone  wishes  keep    state  ignoranc...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledcatdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "cattext_train, cattext_test, catlabel_train, catlabel_test = model_selection.train_test_split(sampledcatdf['clndtxt']\n",
    "                                                                                              ,sampledcatdf['cat'], \n",
    "                                                                                              test_size = 0.25, \n",
    "                                                                                              shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in train 88030\n",
      "Number of observations in train 29344\n"
     ]
    }
   ],
   "source": [
    "# Checking lenghts of test and test\n",
    "print(\"Number of observations in train\", len(cattext_train))\n",
    "\n",
    "print(\"Number of observations in train\", len(cattext_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating count vectorizer objects\n",
    "catcv_vect = CountVectorizer( analyzer='word', \n",
    "                                stop_words = 'english',  # removes english stop words\n",
    "                                ngram_range=(2,3),       # ngrams - 2,3 \n",
    "                                max_features=10000, # Had to restrict to 10000 features otherwise its running forever\n",
    "                                lowercase = True,  \n",
    "                                max_df = 0.5,  \n",
    "                                min_df = 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "catcvtexttrain = catcv_vect.fit_transform(cattext_train) \n",
    "print(len(catcv_vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88030, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(catcv_vect.fit_transform(cattext_train).shape) #(176061, 34779) #(176061, 41095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29344, 10000)\n"
     ]
    }
   ],
   "source": [
    "# converting testing text into matric form for computation using transform\n",
    "catcvtexttest = catcv_vect.transform(cattext_test)\n",
    "print(catcv_vect.transform(cattext_test).shape) #(58687, 34779) # (11738, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88030, 10000)\n",
      "(29344, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Confirming shapes of test and train transformations\n",
    "print(catcvtexttrain.shape)\n",
    "print(catcvtexttest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistric Refression for Categorization Data Set with Penalty L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=-1, penalty='l1',\n",
       "          random_state=0, solver='saga', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catlogregmodel = LogisticRegression(n_jobs=-1, penalty='l1', multi_class = 'multinomial', random_state=0,\n",
    "                                    class_weight=\"balanced\", solver = 'saga')  # solver=\"sag\" for L2\n",
    "\n",
    "# fitting logistic regression model to trained vectors\n",
    "catlogregmodel.fit(catcvtexttrain, catlabel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on trained dataset\n",
    "catlogregmodelpredtr = catlogregmodel.predict(catcvtexttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on test dataset\n",
    "catlogregmodelpredtest = catlogregmodel.predict(catcvtexttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12194\n"
     ]
    }
   ],
   "source": [
    "# Testing total accurate predictions\n",
    "print (np.sum(catlogregmodelpredtest == catlabel_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.26      0.33      5197\n",
      "           1       0.14      0.16      0.15      1941\n",
      "           2       0.39      0.81      0.52      9654\n",
      "           3       0.69      0.22      0.33     12552\n",
      "\n",
      "   micro avg       0.42      0.42      0.42     29344\n",
      "   macro avg       0.42      0.36      0.33     29344\n",
      "weighted avg       0.51      0.42      0.38     29344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate labelled performance metrics\n",
    "\n",
    "print(metrics.classification_report(catlabel_test, catlogregmodelpredtest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4643644212200386 0.4155534351145038\n"
     ]
    }
   ],
   "source": [
    "cataccuracy_train = metrics.accuracy_score(catlabel_train, catlogregmodelpredtr)\n",
    "cataccuracy_test = metrics.accuracy_score(catlabel_test, catlogregmodelpredtest)\n",
    "print(cataccuracy_train, cataccuracy_test) \n",
    "# 0.48422368009996875 0.3903561083659908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dummylist to hold model performance metrics\n",
    "modelperfrows2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_multiclass_metrics(actual_label, predicted_label):\n",
    "    \"\"\"\n",
    "    function that takes acutal labels and predicted labels and returns\n",
    "    accuracy, auc, precision, recall and f1 scores\n",
    "    average = 'weighted' for multi class classification\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(actual_label, predicted_label)\n",
    "    precision = precision_score(actual_label, predicted_label, average = 'weighted')\n",
    "    recall = recall_score(actual_label, predicted_label, average = 'weighted')\n",
    "    f1 = f1_score(actual_label, predicted_label, average = 'weighted')\n",
    "\n",
    "    return (accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4155534351145038,\n",
       " 0.5132254291964905,\n",
       " 0.4155534351145038,\n",
       " 0.3814522093962927)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_multiclass_metrics(catlabel_test, catlogregmodelpredtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4155534351145038,\n",
       " 0.5132254291964905,\n",
       " 0.4155534351145038,\n",
       " 0.3814522093962927)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(fn_metrics(label_test, logregmodelpredtest))\n",
    "\n",
    "acc, prec, recall, f1 = fn_multiclass_metrics(catlabel_test, catlogregmodelpredtest)\n",
    "\n",
    "acc, prec, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression (L1)', 0.4155534351145038, 0.5132254291964905, 0.4155534351145038, 0.3814522093962927]\n"
     ]
    }
   ],
   "source": [
    "l1row= ['Logistic Regression (L1)', acc, prec, recall, f1]\n",
    "print(l1row)\n",
    "\n",
    "modelperfrows2.append(l1row)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistric Regression with L2 Pentality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating logistic regression with penalty L2, multiclass & imbalanced classification \n",
    "catlogregmodel2 = LogisticRegression(n_jobs=-1, penalty='l2', multi_class = 'multinomial', \n",
    "                                     random_state=0, class_weight=\"balanced\", solver = 'sag')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
       "          random_state=0, solver='sag', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting trained data to logistic regression with penalty L2, multiclass & imbalanced classification\n",
    "catlogregmodel2.fit(catcvtexttrain, catlabel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on trained dataset\n",
    "catlogregmodelpredtr2 = catlogregmodel2.predict(catcvtexttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on test dataset\n",
    "catlogregmodelpredtest2 = catlogregmodel2.predict(catcvtexttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12353\n"
     ]
    }
   ],
   "source": [
    "# Testing total accurate predictions\n",
    "print (np.sum(catlogregmodelpredtest2 == catlabel_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.26      0.33      5197\n",
      "           1       0.14      0.17      0.15      1941\n",
      "           2       0.39      0.80      0.52      9654\n",
      "           3       0.69      0.24      0.35     12552\n",
      "\n",
      "   micro avg       0.42      0.42      0.42     29344\n",
      "   macro avg       0.42      0.37      0.34     29344\n",
      "weighted avg       0.51      0.42      0.39     29344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate labelled performance metrics\n",
    "\n",
    "print(metrics.classification_report(catlabel_test, catlogregmodelpredtest2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4867090764512098 0.42097191930207195\n"
     ]
    }
   ],
   "source": [
    "cataccuracy_train2 = metrics.accuracy_score(catlabel_train, catlogregmodelpredtr2)\n",
    "cataccuracy_test2 = metrics.accuracy_score(catlabel_test, catlogregmodelpredtest2)\n",
    "print(cataccuracy_train2, cataccuracy_test2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42097191930207195,\n",
       " 0.5120788817372477,\n",
       " 0.42097191930207195,\n",
       " 0.39222068844269153)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, prec, recall, f1 = fn_multiclass_metrics(catlabel_test, catlogregmodelpredtest2)\n",
    "\n",
    "acc, prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logistic Regression (L2)', 0.42097191930207195, 0.5120788817372477, 0.42097191930207195, 0.39222068844269153]\n"
     ]
    }
   ],
   "source": [
    "# creating a list of accuracy, precision, recall, f1\n",
    "l2row= ['Logistic Regression (L2)', acc, prec, recall, f1]\n",
    "print(l2row)\n",
    "\n",
    "# Appending logistic regression penalty 2 model metrics\n",
    "modelperfrows2.append(l2row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navie Bayes Algorithm for Multi Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5064523457912076 0.4685114503816794\n",
      "13748\n"
     ]
    }
   ],
   "source": [
    "catNb_cv = MultinomialNB(alpha = 1) # added alpha = 1 for laplace smoothing for multi-classification\n",
    "\n",
    "catNb_cv.fit(catcvtexttrain, catlabel_train)\n",
    "\n",
    "# predict the labels on trained dataset\n",
    "catNb_cv_predtr = catNb_cv.predict(catcvtexttrain)\n",
    "\n",
    "# predict the labels on test dataset\n",
    "catNb_cv_predtst = catNb_cv.predict(catcvtexttest)\n",
    "\n",
    "# Calculating Accuracy scores\n",
    "Acc_catNb_cv_predtr = metrics.accuracy_score(catlabel_train, catNb_cv_predtr)\n",
    "Acc_catNb_cv_predtst = metrics.accuracy_score(catlabel_test, catNb_cv_predtst)\n",
    "\n",
    "print(Acc_catNb_cv_predtr, Acc_catNb_cv_predtst) \n",
    "\n",
    "# Testing total accurate predictions for test\n",
    "print (np.sum(catNb_cv_predtst == catlabel_test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.21      0.31      5197\n",
      "           1       0.72      0.02      0.04      1941\n",
      "           2       0.43      0.26      0.32      9654\n",
      "           3       0.47      0.81      0.59     12552\n",
      "\n",
      "   micro avg       0.47      0.47      0.47     29344\n",
      "   macro avg       0.56      0.32      0.32     29344\n",
      "weighted avg       0.50      0.47      0.42     29344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report for test data set using Naive Bayes and CV (for vectorization) algorithms\n",
    "print(metrics.classification_report(catlabel_test, catNb_cv_predtst)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4685114503816794,\n",
       " 0.4985819173945918,\n",
       " 0.4685114503816794,\n",
       " 0.4166923428768707)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, prec, recall, f1 = fn_multiclass_metrics(catlabel_test, catNb_cv_predtst)\n",
    "\n",
    "acc, prec, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Naive Bayes', 0.4685114503816794, 0.4985819173945918, 0.4685114503816794, 0.4166923428768707]\n"
     ]
    }
   ],
   "source": [
    "nbrow= ['Naive Bayes', acc, prec, recall, f1]\n",
    "print(nbrow)\n",
    "\n",
    "modelperfrows2.append(nbrow)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Logistic Regression (L1)',\n",
       "  0.4155534351145038,\n",
       "  0.5132254291964905,\n",
       "  0.4155534351145038,\n",
       "  0.3814522093962927],\n",
       " ['Logistic Regression (L2)',\n",
       "  0.42097191930207195,\n",
       "  0.5120788817372477,\n",
       "  0.42097191930207195,\n",
       "  0.39222068844269153],\n",
       " ['Naive Bayes',\n",
       "  0.4685114503816794,\n",
       "  0.4985819173945918,\n",
       "  0.4685114503816794,\n",
       "  0.4166923428768707]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelperfrows2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data Frame for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (L1)</td>\n",
       "      <td>0.415553</td>\n",
       "      <td>0.513225</td>\n",
       "      <td>0.415553</td>\n",
       "      <td>0.381452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression (L2)</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>0.512079</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>0.392221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.468511</td>\n",
       "      <td>0.498582</td>\n",
       "      <td>0.468511</td>\n",
       "      <td>0.416692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy  Precision    Recall        F1\n",
       "0  Logistic Regression (L1)  0.415553   0.513225  0.415553  0.381452\n",
       "1  Logistic Regression (L2)  0.420972   0.512079  0.420972  0.392221\n",
       "2               Naive Bayes  0.468511   0.498582  0.468511  0.416692"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data frame to hold results\n",
    "multiclass_modelperf_df = pd.DataFrame()\n",
    "multiclass_modelperf_df = pd.DataFrame(modelperfrows2, columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "multiclass_modelperf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower accuracies for Multi-Classificaiton might have something to do \n",
    "    with sampling data set and restricting the maxfeatures to 10000.\n",
    "    \n",
    "Naive Bayes performed better than Logistic Regression in terms of both accuracy, recall and F1, but has lesser precision.\n",
    "\n",
    "Logistic Regression with Penalty L2 has slightly better accuracy than Penalty L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Create k-Fold cross-validation \n",
    "kf3 = KFold(n_splits=3 # 3 fold cross validation\n",
    "            , shuffle=True # to shuffle observations\n",
    "            ,random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, target = sampledcatdf['clndtxt'], sampledcatdf['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', \n",
    "                                stop_words = 'english',  # removes english stop words\n",
    "                                ngram_range=(2,3),       # ngrams - 2,3 \n",
    "                                max_features=10000, \n",
    "                                lowercase = True,  \n",
    "                                max_df = 0.5,  \n",
    "                                min_df = 3)\n",
    "\n",
    "# Converting features into vectorized matrix for applying model\n",
    "features = count_vect.fit_transform(text)\n",
    "print(len(count_vect.get_feature_names())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conduct k-fold cross-validation\n",
    "cv_results = cross_val_score(logregmodel2, # Logistic Regression with penalty L2\n",
    "                            features, # Feature matrix\n",
    "                            target, # Target vector\n",
    "                            cv=kf3, # Cross-validation technique\n",
    "                            scoring=\"f1_weighted\", # for imbalanced multi-class returns average of all labels\n",
    "                            n_jobs=-1) # Use all CPU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39113744 0.39859671 0.39619027]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39530813887574134\n"
     ]
    }
   ],
   "source": [
    "# the cross-validation score of the model’s F1 score using three-fold cross validation\n",
    "print(cv_results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn’s GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# Create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create range of candidate regularization hyperparameter values\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create dictionary hyperparameter candidates\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96711\n"
     ]
    }
   ],
   "source": [
    "text, target = df['clndtxt'], df['con']\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', stop_words='english', lowercase=True, min_df = 2)\n",
    "\n",
    "# Converting features into vectorized matrix for applying model\n",
    "features = count_vect.fit_transform(text)\n",
    "print(len(count_vect.get_feature_names())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l1\n",
      "Best C: 1.0\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9547884215976842"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct nested cross-validation and outut the average score\n",
    "cross_val_score(gridsearch, features, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Installed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.417649</td>\n",
       "      <td>8.344085</td>\n",
       "      <td>0.128166</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1.0, 'penalty': 'l1'}</td>\n",
       "      <td>0.954300</td>\n",
       "      <td>0.954742</td>\n",
       "      <td>0.954921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954766</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956432</td>\n",
       "      <td>0.956404</td>\n",
       "      <td>0.956399</td>\n",
       "      <td>0.956443</td>\n",
       "      <td>0.956389</td>\n",
       "      <td>0.956413</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375.220455</td>\n",
       "      <td>13.539467</td>\n",
       "      <td>0.105151</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1.0, 'penalty': 'l2'}</td>\n",
       "      <td>0.954258</td>\n",
       "      <td>0.954568</td>\n",
       "      <td>0.954742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954618</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956588</td>\n",
       "      <td>0.956533</td>\n",
       "      <td>0.956629</td>\n",
       "      <td>0.956759</td>\n",
       "      <td>0.956593</td>\n",
       "      <td>0.956621</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.468141</td>\n",
       "      <td>11.406862</td>\n",
       "      <td>0.102076</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>2.78256</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 2.7825594022071245, 'penalty': 'l1'}</td>\n",
       "      <td>0.950889</td>\n",
       "      <td>0.951253</td>\n",
       "      <td>0.951858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951601</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>12</td>\n",
       "      <td>0.959680</td>\n",
       "      <td>0.959639</td>\n",
       "      <td>0.959620</td>\n",
       "      <td>0.959666</td>\n",
       "      <td>0.959518</td>\n",
       "      <td>0.959625</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389.289163</td>\n",
       "      <td>37.168921</td>\n",
       "      <td>0.115767</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>2.78256</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 2.7825594022071245, 'penalty': 'l2'}</td>\n",
       "      <td>0.953800</td>\n",
       "      <td>0.954179</td>\n",
       "      <td>0.954311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954162</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>3</td>\n",
       "      <td>0.956980</td>\n",
       "      <td>0.956929</td>\n",
       "      <td>0.957253</td>\n",
       "      <td>0.957395</td>\n",
       "      <td>0.957176</td>\n",
       "      <td>0.957147</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116.183664</td>\n",
       "      <td>13.235475</td>\n",
       "      <td>0.106541</td>\n",
       "      <td>0.011664</td>\n",
       "      <td>7.74264</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 7.742636826811269, 'penalty': 'l1'}</td>\n",
       "      <td>0.946668</td>\n",
       "      <td>0.947116</td>\n",
       "      <td>0.947953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947642</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>13</td>\n",
       "      <td>0.962087</td>\n",
       "      <td>0.962047</td>\n",
       "      <td>0.961933</td>\n",
       "      <td>0.962254</td>\n",
       "      <td>0.961868</td>\n",
       "      <td>0.962038</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>430.987193</td>\n",
       "      <td>29.211520</td>\n",
       "      <td>0.120428</td>\n",
       "      <td>0.020043</td>\n",
       "      <td>7.74264</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 7.742636826811269, 'penalty': 'l2'}</td>\n",
       "      <td>0.953605</td>\n",
       "      <td>0.953826</td>\n",
       "      <td>0.953726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953844</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>4</td>\n",
       "      <td>0.957213</td>\n",
       "      <td>0.957345</td>\n",
       "      <td>0.958109</td>\n",
       "      <td>0.957720</td>\n",
       "      <td>0.957645</td>\n",
       "      <td>0.957606</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>123.808456</td>\n",
       "      <td>18.734265</td>\n",
       "      <td>0.111004</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>21.5443</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 21.544346900318832, 'penalty': 'l1'}</td>\n",
       "      <td>0.943895</td>\n",
       "      <td>0.944395</td>\n",
       "      <td>0.945442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945082</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>14</td>\n",
       "      <td>0.962662</td>\n",
       "      <td>0.962632</td>\n",
       "      <td>0.962611</td>\n",
       "      <td>0.962812</td>\n",
       "      <td>0.962488</td>\n",
       "      <td>0.962641</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>468.162305</td>\n",
       "      <td>47.013286</td>\n",
       "      <td>0.144834</td>\n",
       "      <td>0.023164</td>\n",
       "      <td>21.5443</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 21.544346900318832, 'penalty': 'l2'}</td>\n",
       "      <td>0.953447</td>\n",
       "      <td>0.953963</td>\n",
       "      <td>0.953642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953699</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>7</td>\n",
       "      <td>0.957408</td>\n",
       "      <td>0.957193</td>\n",
       "      <td>0.958150</td>\n",
       "      <td>0.958178</td>\n",
       "      <td>0.958079</td>\n",
       "      <td>0.957802</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>154.357125</td>\n",
       "      <td>28.032086</td>\n",
       "      <td>0.125785</td>\n",
       "      <td>0.012787</td>\n",
       "      <td>59.9484</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 59.94842503189409, 'penalty': 'l1'}</td>\n",
       "      <td>0.943047</td>\n",
       "      <td>0.943411</td>\n",
       "      <td>0.944416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944076</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>15</td>\n",
       "      <td>0.962825</td>\n",
       "      <td>0.962805</td>\n",
       "      <td>0.962807</td>\n",
       "      <td>0.962992</td>\n",
       "      <td>0.962705</td>\n",
       "      <td>0.962827</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>479.928193</td>\n",
       "      <td>23.774045</td>\n",
       "      <td>0.132133</td>\n",
       "      <td>0.027747</td>\n",
       "      <td>59.9484</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 59.94842503189409, 'penalty': 'l2'}</td>\n",
       "      <td>0.953447</td>\n",
       "      <td>0.953863</td>\n",
       "      <td>0.953584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>6</td>\n",
       "      <td>0.957353</td>\n",
       "      <td>0.957266</td>\n",
       "      <td>0.958197</td>\n",
       "      <td>0.958064</td>\n",
       "      <td>0.957984</td>\n",
       "      <td>0.957773</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_C  \\\n",
       "0      94.417649      8.344085         0.128166        0.018739        1   \n",
       "1     375.220455     13.539467         0.105151        0.008630        1   \n",
       "2      88.468141     11.406862         0.102076        0.012127  2.78256   \n",
       "3     389.289163     37.168921         0.115767        0.007892  2.78256   \n",
       "4     116.183664     13.235475         0.106541        0.011664  7.74264   \n",
       "5     430.987193     29.211520         0.120428        0.020043  7.74264   \n",
       "6     123.808456     18.734265         0.111004        0.009997  21.5443   \n",
       "7     468.162305     47.013286         0.144834        0.023164  21.5443   \n",
       "8     154.357125     28.032086         0.125785        0.012787  59.9484   \n",
       "9     479.928193     23.774045         0.132133        0.027747  59.9484   \n",
       "\n",
       "  param_penalty                                      params  \\\n",
       "0            l1                 {'C': 1.0, 'penalty': 'l1'}   \n",
       "1            l2                 {'C': 1.0, 'penalty': 'l2'}   \n",
       "2            l1  {'C': 2.7825594022071245, 'penalty': 'l1'}   \n",
       "3            l2  {'C': 2.7825594022071245, 'penalty': 'l2'}   \n",
       "4            l1   {'C': 7.742636826811269, 'penalty': 'l1'}   \n",
       "5            l2   {'C': 7.742636826811269, 'penalty': 'l2'}   \n",
       "6            l1  {'C': 21.544346900318832, 'penalty': 'l1'}   \n",
       "7            l2  {'C': 21.544346900318832, 'penalty': 'l2'}   \n",
       "8            l1   {'C': 59.94842503189409, 'penalty': 'l1'}   \n",
       "9            l2   {'C': 59.94842503189409, 'penalty': 'l2'}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  ...  \\\n",
       "0           0.954300           0.954742           0.954921  ...   \n",
       "1           0.954258           0.954568           0.954742  ...   \n",
       "2           0.950889           0.951253           0.951858  ...   \n",
       "3           0.953800           0.954179           0.954311  ...   \n",
       "4           0.946668           0.947116           0.947953  ...   \n",
       "5           0.953605           0.953826           0.953726  ...   \n",
       "6           0.943895           0.944395           0.945442  ...   \n",
       "7           0.953447           0.953963           0.953642  ...   \n",
       "8           0.943047           0.943411           0.944416  ...   \n",
       "9           0.953447           0.953863           0.953584  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.954766        0.000343                1            0.956432   \n",
       "1         0.954618        0.000395                2            0.956588   \n",
       "2         0.951601        0.000686               12            0.959680   \n",
       "3         0.954162        0.000436                3            0.956980   \n",
       "4         0.947642        0.001019               13            0.962087   \n",
       "5         0.953844        0.000455                4            0.957213   \n",
       "6         0.945082        0.001179               14            0.962662   \n",
       "7         0.953699        0.000495                7            0.957408   \n",
       "8         0.944076        0.001139               15            0.962825   \n",
       "9         0.953704        0.000476                6            0.957353   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.956404            0.956399            0.956443   \n",
       "1            0.956533            0.956629            0.956759   \n",
       "2            0.959639            0.959620            0.959666   \n",
       "3            0.956929            0.957253            0.957395   \n",
       "4            0.962047            0.961933            0.962254   \n",
       "5            0.957345            0.958109            0.957720   \n",
       "6            0.962632            0.962611            0.962812   \n",
       "7            0.957193            0.958150            0.958178   \n",
       "8            0.962805            0.962807            0.962992   \n",
       "9            0.957266            0.958197            0.958064   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.956389          0.956413         0.000021  \n",
       "1            0.956593          0.956621         0.000076  \n",
       "2            0.959518          0.959625         0.000057  \n",
       "3            0.957176          0.957147         0.000172  \n",
       "4            0.961868          0.962038         0.000133  \n",
       "5            0.957645          0.957606         0.000313  \n",
       "6            0.962488          0.962641         0.000104  \n",
       "7            0.958079          0.957802         0.000416  \n",
       "8            0.962705          0.962827         0.000093  \n",
       "9            0.957984          0.957773         0.000386  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating data frame from cross validation of grid search with various parameters and results\n",
    "results = pd.DataFrame(gridsearch.cv_results_)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
